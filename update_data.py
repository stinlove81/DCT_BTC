import requests
from bs4 import BeautifulSoup
import json
import os
import yfinance as yf
import smtplib
from email.mime.text import MIMEText
from datetime import datetime

def send_mail_report(is_success, error_msg=""):
    gmail_user = os.getenv('MY_GMAIL_USER')
    gmail_pw = os.getenv('MY_GMAIL_PW')
    target_email = "stinlove@kakao.com"

    if not gmail_user or not gmail_pw:
        print("ë©”ì¼ ê³„ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì•Œë¦¼ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    if not is_success:
        # âŒ ì‹¤íŒ¨í–ˆì„ ë•Œë§Œ ë©”ì¼ì„ êµ¬ì„±í•˜ê³  ë°œì†¡í•©ë‹ˆë‹¤.
        subject = f"[DAT MONITOR] âš ï¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({now})"
        body = (
            f"ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤. ì¿ í‚¤ê°€ ë§Œë£Œë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n\n"
            f"âŒ ìƒì„¸ ì›ì¸: {error_msg}\n"
            f"â° ë°œìƒ ì‹œê°„: {now}\n\n"
            f"ì‚¬ì¥ë‹˜, ì‚¬ì´íŠ¸ì—ì„œ ìƒˆ ì¿ í‚¤ë¥¼ ì¶”ì¶œí•´ GitHub Secretsë¥¼ ì—…ë°ì´íŠ¸ í•´ì£¼ì„¸ìš”!"
        )
        
        try:
            msg = MIMEText(body)
            msg['Subject'] = subject
            msg['From'] = gmail_user
            msg['To'] = target_email
            
            with smtplib.SMTP('smtp.gmail.com', 587) as server:
                server.starttls()
                server.login(gmail_user, gmail_pw)
                server.sendmail(gmail_user, target_email, msg.as_string())
            print(f"ğŸš¨ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì•Œë¦¼ ë©”ì¼ì„ ë°œì†¡í–ˆìŠµë‹ˆë‹¤. ({now})")
        except Exception as e:
            print(f"ë©”ì¼ ë°œì†¡ ì—ëŸ¬: {e}")
    else:
        # âœ… ì„±ê³µ ì‹œì—ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
        print(f"âœ… ë°ì´í„° ì—…ë°ì´íŠ¸ ì„±ê³µ ({now}) - ì•Œë¦¼ ë©”ì¼ì„ ë°œì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def scrape_dat_final():
    url = "https://bitcointreasuries.net/"
    # ì‚¬ì¥ë‹˜, ì•„ë˜ ì¿ í‚¤ëŠ” ê³§ ë§Œë£Œë  ìˆ˜ ìˆìœ¼ë‹ˆ ë‚˜ì¤‘ì— ì—ëŸ¬ë‚˜ë©´ ë‹¤ì‹œ ì—…ë°ì´íŠ¸ í•˜ì…”ì•¼ í•©ë‹ˆë‹¤!
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Cookie': '_ga=GA1.1.735990955.1768484196; session=c4e6a4f3-9bae-4368-bb86-cd78f2044956; cf_clearance=MfqJi5nC4zTMmy_ySM_TlepmemDfrGMYd2xXdun8Sxk-1768488123-1.2.1.1-zRB74qLNZgeQmFjt4ovA2rqcqGa_5vBlshf5zmRQBlWeeCLsbA2Vvm0y.B1Asxo2BXlR6JHsbRlp0KqTXfHrvS8IoiTuOI7OqbJsV7tCWLXK3HW5IGbm4A4txlDjvAfOr.c1YZSMqSiz3WmJplo286px856XAmJMtIpnFexNUIitGR4ycHj4nJ8yWpflZKiZasb01gxklTZIGtkBZsrVs0gV5ptFCg9qitkIlkPVZ5Q; _ga_PS4WN3NSZE=GS2.1.s1768487089$o2$g1$t1768488203$j60$l0$h0'
    }

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        all_tables = soup.find_all('table')
        
        base_info = []
        for idx in range(3):
            if idx >= len(all_tables): break
            rows = all_tables[idx].find_all('tr')[1:]
            for row in rows:
                if len(base_info) >= 100: break
                cols = row.find_all(['td', 'th'])
                if len(cols) >= 5:
                    base_info.append({
                        "name": cols[1].get_text(strip=True),
                        "ticker_or_country": cols[3].get_text(strip=True),
                        "btc_holdings": cols[4].get_text(strip=True)
                    })

        extra_info = []
        q_count = 0 
        
        if len(all_tables) >= 4:
            rows_4 = all_tables[3].find_all('tr')[1:]
            for row in rows_4:
                if len(extra_info) >= 100: break
                cols = row.find_all(['td', 'th'])
                if len(cols) >= 7:
                    val1 = cols[5].get_text(strip=True)
                    val2 = cols[6].get_text(strip=True)
                    if '?' in val2:
                        q_count += 1
                    extra_info.append({"extra_val_1": val1, "extra_val_2": val2})

        if len(extra_info) > 10:
            q_ratio = q_count / len(extra_info)
            if q_ratio > 0.8:
                raise Exception(f"EV ë°ì´í„° 80% ì´ìƒ({q_count}ê°œ)ì´ ë¬¼ìŒí‘œë¡œ í‘œì‹œë¨. ì¿ í‚¤ ë§Œë£Œ í™•ì‹¤.")

        combined_data = []
        for i in range(len(base_info)):
            item = base_info[i]
            if i < len(extra_info): item.update(extra_info[i])
            
            ticker = item["ticker_or_country"]
            if ticker.isalpha() and len(ticker) <= 5:
                try:
                    stock = yf.Ticker(ticker)
                    item["live_price"] = round(stock.fast_info['previous_close'], 2)
                except: item["live_price"] = 0
            else: item["live_price"] = 0
            combined_data.append(item)

        file_path = os.path.join(os.path.dirname(__file__), 'data.json')
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=4)
        
        send_mail_report(True)

    except Exception as e:
        send_mail_report(False, str(e))

if __name__ == "__main__":
    scrape_dat_final()
